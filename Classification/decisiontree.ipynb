{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Import Libraries**"
      ],
      "metadata": {
        "id": "NsUnpQC803wB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G0p_58MJ01bI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Load and Display Dataset (Initial Exploratory Data Analysis)**"
      ],
      "metadata": {
        "id": "47ISqwh51Xlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"final_dataset_with_inflation_by_country.csv\")\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Total number of rows in dataset: {len(df)}\\n\")\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "print(\"Dataset description:\")\n",
        "print(df.describe(), \"\\n\")\n",
        "print(\"Dataset info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUuFonqN1c6R",
        "outputId": "c49d5ff9-9a6a-4655-8d48-30fd8e84f39f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows in dataset: 3671\n",
            "\n",
            "First few rows of the dataset:\n",
            "   Month  Year         Country  Departures  Arrivals  Quarter  \\\n",
            "0      1  2000  United Kingdom     46677.0   45630.0        1   \n",
            "1      1  2000         Germany     23172.0   24251.0        1   \n",
            "2      1  2000          France     11098.0   11469.0        1   \n",
            "3      1  2000          Europe    118255.0  119735.0        1   \n",
            "4      1  2000         Oceania    139244.0  133358.0        1   \n",
            "\n",
            "   Years_Since_2000  Departures_Growth  Arrivals_Growth  Total Holidays  \\\n",
            "0                 0                0.0              0.0               2   \n",
            "1                 0                0.0              0.0               2   \n",
            "2                 0                0.0              0.0               2   \n",
            "3                 0                0.0              0.0               2   \n",
            "4                 0                0.0              0.0               2   \n",
            "\n",
            "   Holiday Ratio  Inflation  \n",
            "0       0.064516        0.8  \n",
            "1       0.064516        1.4  \n",
            "2       0.064516        1.8  \n",
            "3       0.064516        1.8  \n",
            "4       0.064516        1.8   \n",
            "\n",
            "Dataset description:\n",
            "             Month         Year    Departures      Arrivals      Quarter  \\\n",
            "count  3671.000000  3671.000000  3.671000e+03  3.671000e+03  3671.000000   \n",
            "mean      6.501498  2010.649959  1.732550e+05  1.748953e+05     2.500409   \n",
            "std       3.451799     5.496722  2.017907e+05  2.047224e+05     1.118064   \n",
            "min       1.000000  2000.000000  1.014600e+04  9.773000e+03     1.000000   \n",
            "25%       4.000000  2007.000000  5.023300e+04  5.086050e+04     2.000000   \n",
            "50%       7.000000  2011.000000  1.171710e+05  1.178350e+05     3.000000   \n",
            "75%       9.500000  2015.000000  2.056755e+05  2.049995e+05     3.500000   \n",
            "max      12.000000  2019.000000  1.352214e+06  1.354850e+06     4.000000   \n",
            "\n",
            "       Years_Since_2000  Departures_Growth  Arrivals_Growth  Total Holidays  \\\n",
            "count       3671.000000        3671.000000      3671.000000     3671.000000   \n",
            "mean          10.649959           0.012859         0.012899        0.999728   \n",
            "std            5.496722           0.130222         0.131508        0.970266   \n",
            "min            0.000000          -0.400179        -0.364426        0.000000   \n",
            "25%            7.000000          -0.068331        -0.075013        0.000000   \n",
            "50%           11.000000           0.011770         0.001086        1.000000   \n",
            "75%           15.000000           0.087934         0.093385        2.000000   \n",
            "max           19.000000           0.560777         1.033633        5.000000   \n",
            "\n",
            "       Holiday Ratio    Inflation  \n",
            "count    3671.000000  3671.000000  \n",
            "mean        0.032837     2.687142  \n",
            "std         0.031900     3.231009  \n",
            "min         0.000000    -1.800000  \n",
            "25%         0.000000     0.600000  \n",
            "50%         0.032258     1.900000  \n",
            "75%         0.064516     3.500000  \n",
            "max         0.161290    23.100000   \n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3671 entries, 0 to 3670\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   Month              3671 non-null   int64  \n",
            " 1   Year               3671 non-null   int64  \n",
            " 2   Country            3671 non-null   object \n",
            " 3   Departures         3671 non-null   float64\n",
            " 4   Arrivals           3671 non-null   float64\n",
            " 5   Quarter            3671 non-null   int64  \n",
            " 6   Years_Since_2000   3671 non-null   int64  \n",
            " 7   Departures_Growth  3671 non-null   float64\n",
            " 8   Arrivals_Growth    3671 non-null   float64\n",
            " 9   Total Holidays     3671 non-null   int64  \n",
            " 10  Holiday Ratio      3671 non-null   float64\n",
            " 11  Inflation          3671 non-null   float64\n",
            "dtypes: float64(6), int64(5), object(1)\n",
            "memory usage: 344.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Define Features and Target Variable**"
      ],
      "metadata": {
        "id": "auCIjBtQ1iXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target variable (y)\n",
        "X = df.drop(columns=['Country'])\n",
        "y = df['Country']"
      ],
      "metadata": {
        "id": "n9XQK30Q1lIh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Train-Test Split**"
      ],
      "metadata": {
        "id": "Sndvbbbl1oeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "7A0Ep5l51qks"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Feature Selection on the Training Set**"
      ],
      "metadata": {
        "id": "jGFSjkgT1sDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a base Decision Tree to determine feature importances\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "base_model.fit(X_train, y_train)\n",
        "importances = base_model.feature_importances_\n",
        "\n",
        "# Set threshold for selecting features\n",
        "threshold = 0.1\n",
        "selected_features = X.columns[importances > threshold]\n",
        "print(f\"\\nSelected features (importance > {threshold}): {selected_features.tolist()}\")\n",
        "\n",
        "# Create new training and test sets with only the selected features\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwgihpYu1u1I",
        "outputId": "8ff7fe6b-1ce3-420a-de0c-88ea1fe3f811"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected features (importance > 0.1): ['Departures', 'Arrivals', 'Years_Since_2000', 'Inflation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Train and Evaluate the Untuned Model**"
      ],
      "metadata": {
        "id": "TSj_bWNJ1wnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Decision Tree Classifier\n",
        "untuned_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the untuned model\n",
        "untuned_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy_untuned = untuned_model.score(X_train_selected, y_train)\n",
        "test_accuracy_untuned = accuracy_score(y_test, untuned_model.predict(X_test_selected))\n",
        "cv_scores_untuned = cross_val_score(estimator=untuned_model, X=X_train_selected, y=y_train, cv=10)\n",
        "cm_untuned = confusion_matrix(y_test, untuned_model.predict(X_test_selected))\n",
        "classification_report_untuned = classification_report(y_test, untuned_model.predict(X_test_selected))\n",
        "\n",
        "# Display results\n",
        "print(\"\\nUntuned Model Evaluation:\")\n",
        "print(f\"Training Accuracy: {train_accuracy_untuned:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy_untuned:.4f}\")\n",
        "print(f\"10-Fold CV Accuracy: Mean = {cv_scores_untuned.mean():.4f}, Std = {cv_scores_untuned.std():.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{cm_untuned}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report_untuned}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gymoviyO1y9T",
        "outputId": "02adc034-7355-49cf-fbe4-37726915d63a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Untuned Model Evaluation:\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy: 0.7441\n",
            "10-Fold CV Accuracy: Mean = 0.7493, Std = 0.0265\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36  0  0  2  0  0  5  0  0  0  0  5  0 13  0  3  0  0]\n",
            " [ 0 52  0  0  0  0  0  0  5  4  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 49  0  0  0  0  0  4  7  0  0  0  0  0  0  2  0]\n",
            " [ 1  0  0 41  0 15  6  2  0  0  0  0  3  4  0  1  0  1]\n",
            " [ 2  0  0  0 49  0  6  1  0  0  1  2  0  2  0  0  0  0]\n",
            " [ 0  0  0  7  0 45  0  0  1  0  0  0  2  0  0  0  0  0]\n",
            " [ 2  0  0  5  3  2 28  2  0  1  0  2  0 10  0  1  0  0]\n",
            " [ 0  0  0  1  0  0  4 42  0  0  0  2  0  0  0  2  0  0]\n",
            " [ 0 10  6  0  0  1  0  0 47  2  0  0  1  0  0  0  1  0]\n",
            " [ 0  3  7  0  0  0  0  0  7 38  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 57  1  0  0  2  0  0  0]\n",
            " [ 7  0  0  0  3  0  1  2  0  0  0 33  0 11  0  2  0  0]\n",
            " [ 0  0  0  2  0  3  0  0  0  2  0  0 47  0  0  0  0  4]\n",
            " [ 6  0  0  4  0  0  8  1  0  0  0  6  0 33  0  3  1  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0 57  0  0  0]\n",
            " [ 2  0  0  1  0  0  3  4  0  0  0  1  0  0  0 59  0  0]\n",
            " [ 0  0  1  0  0  3  0  0  7  0  0  0  0  1  0  0 57  0]\n",
            " [ 0  0  0  3  0  0  0  0  0  1  0  0  3  0  0  0  0 50]]\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Europe       0.64      0.56      0.60        64\n",
            "         France       0.80      0.85      0.83        61\n",
            "        Germany       0.78      0.79      0.78        62\n",
            "      Hong Kong       0.62      0.55      0.59        74\n",
            "      Indonesia       0.89      0.78      0.83        63\n",
            "          Japan       0.65      0.82      0.73        55\n",
            " Mainland China       0.46      0.50      0.48        56\n",
            "       Malaysia       0.78      0.82      0.80        51\n",
            "    Middle East       0.66      0.69      0.68        68\n",
            "  North America       0.69      0.69      0.69        55\n",
            "North East Asia       0.97      0.95      0.96        60\n",
            "        Oceania       0.63      0.56      0.59        59\n",
            "    Philippines       0.84      0.81      0.82        58\n",
            "     South Asia       0.45      0.53      0.49        62\n",
            "South East Asia       0.97      0.98      0.97        58\n",
            "       Thailand       0.83      0.84      0.84        70\n",
            " United Kingdom       0.93      0.83      0.88        69\n",
            "        Vietnam       0.91      0.88      0.89        57\n",
            "\n",
            "       accuracy                           0.74      1102\n",
            "      macro avg       0.75      0.75      0.75      1102\n",
            "   weighted avg       0.75      0.74      0.75      1102\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Hyperparameter Tuning (Using Cross-Validation on the Training Set)**"
      ],
      "metadata": {
        "id": "U5TGdjON12nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [6, 8, 10],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=untuned_model, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# Display best parameters and score\n",
        "print(\"\\nGridSearchCV Results:\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oWPNlNZ15WW",
        "outputId": "328b0c02-4f9b-4f92-dbb3-ad8a6cd947ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GridSearchCV Results:\n",
            "Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_weight_fraction_leaf': 0.0, 'splitter': 'best'}\n",
            "Best Cross-Validation Score: 0.7045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Train and Evaluate the Tuned Model**"
      ],
      "metadata": {
        "id": "dnBGQ5K-17Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best estimator\n",
        "tuned_model = grid_search.best_estimator_\n",
        "\n",
        "# Train the tuned model\n",
        "tuned_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "train_accuracy_tuned = tuned_model.score(X_train_selected, y_train)\n",
        "test_accuracy_tuned = accuracy_score(y_test, tuned_model.predict(X_test_selected))\n",
        "cv_scores_tuned = cross_val_score(estimator=tuned_model, X=X_train_selected, y=y_train, cv=10)\n",
        "cm_tuned = confusion_matrix(y_test, tuned_model.predict(X_test_selected))\n",
        "classification_report_tuned = classification_report(y_test, tuned_model.predict(X_test_selected))\n",
        "\n",
        "# Display results\n",
        "print(\"\\nTuned Model Evaluation:\")\n",
        "print(f\"Training Accuracy: {train_accuracy_tuned:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy_tuned:.4f}\")\n",
        "print(f\"10-Fold CV Accuracy: Mean = {cv_scores_tuned.mean():.4f}, Std = {cv_scores_tuned.std():.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{cm_tuned}\")\n",
        "print(f\"\\nClassification Report:\\n{classification_report_tuned}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUF_Cirb1_-g",
        "outputId": "6b0f21bd-49dd-407b-9452-3ba8b3a7b669"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tuned Model Evaluation:\n",
            "Training Accuracy: 0.8571\n",
            "Testing Accuracy: 0.7305\n",
            "10-Fold CV Accuracy: Mean = 0.7049, Std = 0.0248\n",
            "\n",
            "Confusion Matrix:\n",
            "[[24  0  0  4  1  0  3  1  0  0  0 11  0 18  0  2  0  0]\n",
            " [ 0 59  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 55  0  0  0  0  0  1  6  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0 31  0 14 10  2  0  0  0  0  9  4  0  1  1  0]\n",
            " [ 1  0  0  0 54  0  2  1  0  0  0  3  0  1  0  1  0  0]\n",
            " [ 0  0  0 13  0 41  0  0  0  0  0  0  0  0  0  0  0  1]\n",
            " [ 2  0  0  3  2  1 25  5  0  2  0  1  0 13  0  2  0  0]\n",
            " [ 1  0  0  1  3  0  3 38  0  0  0  3  0  1  0  1  0  0]\n",
            " [ 0 11  2  0  0  0  0  0 47  6  0  0  2  0  0  0  0  0]\n",
            " [ 0  2  7  0  0  0  0  0  9 36  0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 58  1  0  0  1  0  0  0]\n",
            " [ 6  0  0  0  2  0  1  2  0  0  0 44  0  3  0  1  0  0]\n",
            " [ 3  0  0  2  0  2  4  0  0  0  0  0 43  0  0  0  0  4]\n",
            " [10  0  0  3  1  0  6  1  0  0  0  5  1 35  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 58  0  0  0]\n",
            " [ 1  0  0  4  1  0  3 17  0  0  0  4  0  0  0 40  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0 64  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0 53]]\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "         Europe       0.48      0.38      0.42        64\n",
            "         France       0.82      0.97      0.89        61\n",
            "        Germany       0.86      0.89      0.87        62\n",
            "      Hong Kong       0.51      0.42      0.46        74\n",
            "      Indonesia       0.84      0.86      0.85        63\n",
            "          Japan       0.71      0.75      0.73        55\n",
            " Mainland China       0.44      0.45      0.44        56\n",
            "       Malaysia       0.57      0.75      0.64        51\n",
            "    Middle East       0.76      0.69      0.72        68\n",
            "  North America       0.69      0.65      0.67        55\n",
            "North East Asia       1.00      0.97      0.98        60\n",
            "        Oceania       0.61      0.75      0.67        59\n",
            "    Philippines       0.73      0.74      0.74        58\n",
            "     South Asia       0.47      0.56      0.51        62\n",
            "South East Asia       0.98      1.00      0.99        58\n",
            "       Thailand       0.83      0.57      0.68        70\n",
            " United Kingdom       0.98      0.93      0.96        69\n",
            "        Vietnam       0.90      0.93      0.91        57\n",
            "\n",
            "       accuracy                           0.73      1102\n",
            "      macro avg       0.73      0.74      0.73      1102\n",
            "   weighted avg       0.73      0.73      0.73      1102\n",
            "\n"
          ]
        }
      ]
    }
  ]
}